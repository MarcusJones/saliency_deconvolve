{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded modules:\n",
      "numpy                np              1.14.2\n",
      "pandas               pd              0.22.0\n",
      "sklearn              sk              0.19.1\n",
      "keras                ks              2.1.5\n",
      "\n",
      "matplotlib           mpl             2.2.2\n",
      "matplotlib.pyplot    plt             N/A\n",
      "matplotlib.image     mpimg           N/A\n",
      "seaborn              sns             0.8.1\n",
      "PIL                  PIL             5.1.0\n",
      "\n",
      "ExergyUtilities      exergy          2.0.\n",
      "\n",
      "pyspark              NOT LOADED      N/A\n"
     ]
    }
   ],
   "source": [
    "print_imports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from keras.layers import Input, Dense, merge\n",
    "from keras.models import Model\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Reshape, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "\n",
    "from glob import iglob\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight file: /home/batman/d2testing/data/tub_73_18-05-02/mypilot73.h5\n",
      "/home/batman/d2testing/data/tub_73_18-05-02/imgs/*.jpg\n",
      "3750 Images found in /home/batman/d2testing/data/tub_73_18-05-02/imgs\n"
     ]
    }
   ],
   "source": [
    "#path_root = r\"/home/batman/d2/data/tub_30\"\n",
    "#path_root = r\"/home/batman/d2/data/tub_47\"\n",
    "#path_root = r\"/home/batman/d2/data/tub_52\"\n",
    "#path_root = r\"/home/batman/d2/data/tub_67\"\n",
    "path_root = r\"/home/batman/d2/data/tub_67\"\n",
    "\n",
    "path_root = r\"/home/batman/d2testing/data/tub_73_18-05-02\"\n",
    "\n",
    "#/home/batman/d2testing/data/tub_73_18-05-02/imgs\n",
    "\n",
    "path_images_dir = os.path.join(path_root,'imgs')\n",
    "path_weights = os.path.join(path_root,'mypilot73.h5')\n",
    "\n",
    "assert os.path.exists(path_weights), path_weights\n",
    "assert os.path.exists(path_images_dir)\n",
    "print('Weight file:',path_weights)\n",
    "\n",
    "glob_string = path_images_dir + '/*.jpg'\n",
    "print(glob_string)\n",
    "all_images = sorted(iglob(glob_string))\n",
    "print(len(all_images), 'Images found in', path_images_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pillow', 'imagemagick', 'imagemagick_file', 'html']\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "print(mpl.animation.writers.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_categorical():\n",
    "    img_in = Input(shape=(120, 160, 3), name='img_in')                      # First layer, input layer, Shape comes from camera.py resolution, RGB\n",
    "    x = img_in\n",
    "    x = Convolution2D(24, (5,5), strides=(2,2), activation='relu', name = 'conv1')(x)       # 24 features, 5 pixel x 5 pixel kernel (convolution, feauture) window, 2wx2h stride, relu activation\n",
    "    x = Convolution2D(32, (5,5), strides=(2,2), activation='relu', name = 'conv2')(x)       # 32 features, 5px5p kernel window, 2wx2h stride, relu activatiion\n",
    "    x = Convolution2D(64, (5,5), strides=(2,2), activation='relu', name = 'conv3')(x)       # 64 features, 5px5p kernal window, 2wx2h stride, relu\n",
    "    x = Convolution2D(64, (3,3), strides=(2,2), activation='relu', name = 'conv4')(x)       # 64 features, 3px3p kernal window, 2wx2h stride, relu\n",
    "    x = Convolution2D(64, (3,3), strides=(1,1), activation='relu', name = 'conv5')(x)       # 64 features, 3px3p kernal window, 1wx1h stride, relu\n",
    "\n",
    "    # Possibly add MaxPooling (will make it less sensitive to position in image).  Camera angle fixed, so may not to be needed\n",
    "\n",
    "    x = Flatten(name='flattened')(x)                                        # Flatten to 1D (Fully connected)\n",
    "    x = Dense(100, activation='relu', name = 'dense1')(x)                                    # Classify the data into 100 features, make all negatives 0\n",
    "    x = Dropout(.1)(x)                                                      # Randomly drop out (turn off) 10% of the neurons (Prevent overfitting)\n",
    "    x = Dense(50, activation='relu', name = 'dense2')(x)                                     # Classify the data into 50 features, make all negatives 0\n",
    "    x = Dropout(.1)(x)                                                      # Randomly drop out 10% of the neurons (Prevent overfitting)\n",
    "    #categorical output of the angle\n",
    "    angle_out = Dense(15, activation='softmax', name='angle_out')(x)        # Connect every input with every output and output 15 hidden units. Use Softmax to give percentage. 15 categories and find best one based off percentage 0.0-1.0\n",
    "    \n",
    "    #continous output of throttle\n",
    "    throttle_out = Dense(1, activation='relu', name='throttle_out')(x)      # Reduce to 1 number, Positive number only\n",
    "    \n",
    "    model = Model(inputs=[img_in], outputs=[angle_out, throttle_out])\n",
    "  \n",
    "    return model\n",
    "\n",
    "def compute_visualisation_mask(img):\n",
    "    activations = functor([np.array([img])])\n",
    "    upscaled_activation = np.ones((3, 6))\n",
    "    for layer in [5, 4, 3, 2, 1]:\n",
    "        averaged_activation = np.mean(activations[layer], axis=3).squeeze(axis=0) * upscaled_activation\n",
    "        output_shape = (activations[layer - 1].shape[1], activations[layer - 1].shape[2])\n",
    "        x = tf.constant(\n",
    "            np.reshape(averaged_activation, (1,averaged_activation.shape[0],averaged_activation.shape[1],1)),\n",
    "            tf.float32\n",
    "        )\n",
    "        conv = tf.nn.conv2d_transpose(\n",
    "            x, layers_kernels[layer],\n",
    "            output_shape=(1,output_shape[0],output_shape[1], 1), \n",
    "            strides=layers_strides[layer], \n",
    "            padding='VALID'\n",
    "        )\n",
    "        with tf.Session() as session:\n",
    "            result = session.run(conv)\n",
    "        upscaled_activation = np.reshape(result, output_shape)\n",
    "    final_visualisation_mask = upscaled_activation\n",
    "    return (final_visualisation_mask - np.min(final_visualisation_mask))/(np.max(final_visualisation_mask) - np.min(final_visualisation_mask))\n",
    "\n",
    "def plot_movie_mp4(image_array):\n",
    "    dpi = 72.0\n",
    "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
    "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi)\n",
    "    im = plt.figimage(image_array[0])\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_array(image_array[i])\n",
    "        return (im,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array))\n",
    "    display(HTML(anim.to_html5_video()))\n",
    "\n",
    "def save_movie_mp4(image_array,path):\n",
    "    dpi = 72.0\n",
    "    xpixels, ypixels = image_array[0].shape[0], image_array[0].shape[1]\n",
    "    fig = plt.figure(figsize=(ypixels/dpi, xpixels/dpi), dpi=dpi);\n",
    "    im = plt.figimage(image_array[0]);\n",
    "\n",
    "    def animate(i):\n",
    "        im.set_array(image_array[i])\n",
    "        return (im,)\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(image_array));\n",
    "    anim.save(path);\n",
    "    print('* SAVED to ', path, '******')\n",
    "    pass\n",
    "\n",
    "def gen_image_array(glob_string, mark_dict):\n",
    "    gen_start = time.time()\n",
    "    imgs = []\n",
    "    counter = 0\n",
    "    start_image_nr = mark_dict['start_frame']\n",
    "    end_image_nr = mark_dict['end_frame']\n",
    "    total_images = end_image_nr - start_image_nr\n",
    "    img_paths = sorted(iglob(glob_string))\n",
    "    #for path in sorted(iglob('imgs/*.jpg')):\n",
    "    for path in img_paths[start_image_nr:]:\n",
    "        cnt_remaining = total_images - counter\n",
    "        \n",
    "        img = cv2.imread(path)\n",
    "        imgs.append(img)\n",
    "        counter += 1\n",
    "        if counter+start_image_nr >= end_image_nr:\n",
    "            break\n",
    "\n",
    "    print(\"* DONE normal gen with {} frames, over {} minutes *******\".format(counter,(gen_start - time.time())/60))\n",
    "    return imgs    \n",
    "\n",
    "def gen_blended_frames(glob_string, mark_dict):\n",
    "    #mark_dict['fname'] = \"{} \n",
    "    #mark_dict['path'] = os.pa\n",
    "    #mark_dict['start_frame'] \n",
    "    #mark_dict['end_frame'] = \n",
    "    print('* STARTING BLENDED FRAME GEN *********')\n",
    "    gen_start = time.time()\n",
    "    imgs = []\n",
    "    alpha = 0.004\n",
    "    beta = 1.0 - alpha\n",
    "    counter = 0\n",
    "    start_image_nr = mark_dict['start_frame']\n",
    "    end_image_nr = mark_dict['end_frame']\n",
    "    total_images = end_image_nr - start_image_nr\n",
    "    img_paths = sorted(iglob(glob_string))\n",
    "    #for path in sorted(iglob('imgs/*.jpg')):\n",
    "    for path in img_paths[start_image_nr:]:\n",
    "        start = time.time()\n",
    "        cnt_remaining = max_images - counter\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "\n",
    "        salient_mask = compute_visualisation_mask(img)\n",
    "        salient_mask_stacked = np.dstack((salient_mask,salient_mask))\n",
    "        salient_mask_stacked = np.dstack((salient_mask_stacked,salient_mask))\n",
    "\n",
    "        blend = cv2.addWeighted(img.astype('float32'), alpha, salient_mask_stacked, beta, 0.0)\n",
    "        imgs.append(blend)\n",
    "        stop = time.time()\n",
    "        duration = stop-start\n",
    "        #print(counter%20)\n",
    "        if counter%10 == 0:\n",
    "            print(\"{:>4} of {} {}\".format(counter, total_images, os.path.split(path)[1]), end = \"\")\n",
    "            print(\", {:.1f}s, \".format(duration), end=\"\")\n",
    "            print(\"{:.1f} min remaining\".format(duration/60*(cnt_remaining-1)))\n",
    "\n",
    "        counter += 1\n",
    "        if counter+start_image_nr >= end_image_nr:\n",
    "            break\n",
    "\n",
    "    print(\"* DONE with {} frames, over {} minutes *******\".format(counter,(gen_start - time.time())/60))\n",
    "    return imgs\n",
    "\n",
    "def gen_salience_only_frames(glob_string, mark_dict):\n",
    "    print('* STARTING SALIENCE FRAME GEN *********')\n",
    "    gen_start = time.time()\n",
    "    imgs = []\n",
    "    alpha = 0.004\n",
    "    beta = 1.0 - alpha\n",
    "    counter = 0\n",
    "    start_image_nr = mark_dict['start_frame']\n",
    "    end_image_nr = mark_dict['end_frame']\n",
    "    total_images = end_image_nr - start_image_nr\n",
    "    img_paths = sorted(iglob(glob_string))\n",
    "    #for path in sorted(iglob('imgs/*.jpg')):\n",
    "    for path in img_paths[start_image_nr:]:\n",
    "        start = time.time()\n",
    "        cnt_remaining = total_images - counter\n",
    "\n",
    "        img = cv2.imread(path)\n",
    "\n",
    "        salient_mask = compute_visualisation_mask(img)\n",
    "        salient_mask_stacked = np.dstack((salient_mask,salient_mask))\n",
    "        salient_mask_stacked = np.dstack((salient_mask_stacked,salient_mask))\n",
    "\n",
    "        #blend = cv2.addWeighted(img.astype('float32'), alpha, salient_mask_stacked, beta, 0.0)\n",
    "        imgs.append(salient_mask_stacked)\n",
    "        stop = time.time()\n",
    "        duration = stop-start\n",
    "        #print(counter%20)\n",
    "        if counter%10 == 0:\n",
    "            print(\"{:>4} of {} {}\".format(counter, total_images, os.path.split(path)[1]), end = \"\")\n",
    "            print(\", {:.1f}s, \".format(duration), end=\"\")\n",
    "            print(\"{:.1f} min remaining\".format(duration/60*(cnt_remaining-1)))\n",
    "\n",
    "        counter += 1\n",
    "        if counter+start_image_nr >= end_image_nr:\n",
    "            break\n",
    "\n",
    "    print(\"* DONE with {} frames, over {} minutes *******\".format(counter,(gen_start - time.time())/60))\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Images in MatPlotLib\n",
    "path_img1 = os.path.join(path_images_dir, all_images[0])\n",
    "path_img_last = os.path.join(path_images_dir, all_images[-1])\n",
    "path_img_last\n",
    "this_img=mpimg.imread(path_img1)\n",
    "imgplot = plt.imshow(this_img)\n",
    "plt.show()\n",
    "\n",
    "# Images in cv2\n",
    "im = cv2.imread(path_img1)\n",
    "im_resized = cv2.resize(im, (500, 500), interpolation=cv2.INTER_LINEAR)\n",
    "print(type(im_resized))\n",
    "print(im_resized.shape)\n",
    "im_resized = im\n",
    "plt.imshow(cv2.cvtColor(im_resized, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Images in Image\n",
    "import Image\n",
    "\n",
    "image_Image = Image.open(path_img1)\n",
    "image_Image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = default_categorical()\n",
    "model.load_weights(path_weights)\n",
    "#model.load_weights('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_in = Input(shape=(120, 160, 3), name='img_in')\n",
    "x = img_in\n",
    "x = Convolution2D(24, (5,5), strides=(2,2), activation='relu', name='conv1')(x)\n",
    "x = Convolution2D(32, (5,5), strides=(2,2), activation='relu', name='conv2')(x)\n",
    "x = Convolution2D(64, (5,5), strides=(2,2), activation='relu', name='conv3')(x)\n",
    "x = Convolution2D(64, (3,3), strides=(2,2), activation='relu', name='conv4')(x)\n",
    "conv_5 = Convolution2D(64, (3,3), strides=(1,1), activation='relu', name='conv5')(x)\n",
    "convolution_part = Model(inputs=[img_in], outputs=[conv_5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer_num in ('1', '2', '3', '4', '5'):\n",
    "    convolution_part.get_layer('conv' + layer_num).set_weights(model.get_layer('conv' + layer_num).get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = convolution_part.input                                           # input placeholder\n",
    "outputs = [layer.output for layer in convolution_part.layers]          # all layer outputs\n",
    "functor = K.function([inp], outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel_3x3 = tf.constant(np.array([\n",
    "        [[[1]], [[1]], [[1]]], \n",
    "        [[[1]], [[1]], [[1]]], \n",
    "        [[[1]], [[1]], [[1]]]\n",
    "]), tf.float32)\n",
    "\n",
    "kernel_5x5 = tf.constant(np.array([\n",
    "        [[[1]], [[1]], [[1]], [[1]], [[1]]], \n",
    "        [[[1]], [[1]], [[1]], [[1]], [[1]]], \n",
    "        [[[1]], [[1]], [[1]], [[1]], [[1]]],\n",
    "        [[[1]], [[1]], [[1]], [[1]], [[1]]],\n",
    "        [[[1]], [[1]], [[1]], [[1]], [[1]]]\n",
    "]), tf.float32)\n",
    "\n",
    "layers_kernels = {5: kernel_3x3, 4: kernel_3x3, 3: kernel_5x5, 2: kernel_5x5, 1: kernel_5x5}\n",
    "\n",
    "layers_strides = {5: [1, 1, 1, 1], 4: [1, 2, 2, 1], 3: [1, 2, 2, 1], 2: [1, 2, 2, 1], 1: [1, 2, 2, 1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false\n",
    "# Frames to left and right\n",
    "# Laptime is say 180 frames\n",
    "# Therefore, take 90 \n",
    "frame_window = 90\n",
    "\n",
    "markers = [\n",
    "    #(\"Off track corner CW\" , 330),\n",
    "    #(\"Start CW\" , 499 ),\n",
    "    #(\"Success CW\" , 796),\n",
    "    #(\"Success CW\" , 1040 ),\n",
    "    #(\"Success CW\" , 1304 ),\n",
    "    #(\"Success CW\" , 1560 ),\n",
    "    #(\"Off track corner CW\" , 1770 ),\n",
    "    #(\"Start CW\" , 1870 ),\n",
    "    #(\"Success CW\" , 2118 ),\n",
    "    #(\"Success CW\" , 2355  ),\n",
    "    (\"Start CCW\" , 2562 ),\n",
    "    (\"Start CW\" ,     3378 ),\n",
    "    (\"Off track corner CW\" , 3480 ),\n",
    "    (\"Start CW\" , 3650 ),\n",
    "    (\"Off track corner CW\" , 3790 ),\n",
    "    (\"Start CW\" , 3890 ),\n",
    "    (\"Off track corner CW\" , 4100 ),\n",
    "    (\"Start CCW\" , 4239 ),\n",
    "    (\"Off track corner CCW \" , 4516 ),\n",
    "    (\"Start CCW\" , 4600 ),\n",
    "    (\"Success on corner CCW\" , 4825 ),\n",
    "    (\"Off track corner CCW\" , 5029 ),\n",
    "    (\"Start CCW\" , 5092 ),\n",
    "    (\"Off track corner CCW\" , 5270 ),\n",
    "    (\"Start CCW\" , 5343 ),\n",
    "    (\"Success on corner CCW\" , 5490 ),\n",
    "    (\"Success on corner CCW\" , 5707 ),\n",
    "    (\"Success on corner CCW\" , 6866 ),\n",
    "    (\"Success on corner CCW\" , 7047 ),\n",
    "    (\"Off track corner CCW\" , 7587 ),\n",
    "]\n",
    "\n",
    "markers = [\n",
    "    (\"Off track corner CCW \" , 4516 ),\n",
    "    (\"Success on corner CCW\" , 4825 ),\n",
    "]\n",
    "\n",
    "print(len(markers))\n",
    "\n",
    "marker_dicts = list()\n",
    "for mark in markers:\n",
    "    mark_dict = dict()\n",
    "    mark_dict['norm_fname'] = \"norm {} {}.mp4\".format(mark[1],mark[0])\n",
    "    mark_dict['norm_vid_path'] = os.path.join(path_root,mark_dict['norm_fname'])\n",
    "    mark_dict['ai_fname'] = \"ai {} {}.mp4\".format(mark[1],mark[0])\n",
    "    mark_dict['ai_vid_path'] = os.path.join(path_root,mark_dict['ai_fname'])\n",
    "    mark_dict['start_frame'] = mark[1] - frame_window\n",
    "    mark_dict['end_frame'] = mark[1] + frame_window\n",
    "    \n",
    "    marker_dicts.append(mark_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "{'norm_fname': 'long normal.mp4', 'norm_vid_path': '/home/batman/d2testing/data/tub_73_18-05-02/long normal.mp4', 'ai_fname': 'long ai.mp4', 'ai_vid_path': '/home/batman/d2testing/data/tub_73_18-05-02/long ai.mp4', 'start_frame': 1000, 'end_frame': 1100}\n"
     ]
    }
   ],
   "source": [
    "# For a big final video\n",
    "marker_dicts = list()\n",
    "mark_dict = dict()\n",
    "mark_dict['norm_fname'] = \"long normal.mp4\"\n",
    "mark_dict['norm_vid_path'] = os.path.join(path_root,mark_dict['norm_fname'])\n",
    "mark_dict['ai_fname'] = \"long ai.mp4\"\n",
    "mark_dict['ai_vid_path'] = os.path.join(path_root,mark_dict['ai_fname'])\n",
    "mark_dict['start_frame'] = 1000\n",
    "mark_dict['end_frame'] = 1100\n",
    "print(mark_dict['end_frame']-mark_dict['start_frame'])\n",
    "marker_dicts.append(mark_dict)\n",
    "     \n",
    "print(marker_dicts[-1])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter ffmpeg unavailable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* PROCESSING long normal.mp4\n",
      "{'norm_fname': 'long normal.mp4', 'norm_vid_path': '/home/batman/d2testing/data/tub_73_18-05-02/long normal.mp4', 'ai_fname': 'long ai.mp4', 'ai_vid_path': '/home/batman/d2testing/data/tub_73_18-05-02/long ai.mp4', 'start_frame': 1000, 'end_frame': 1100}\n",
      "* DONE normal gen with 100 frames, over -0.0010422309239705404 minutes *******\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unknown file extension: .mp4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1914\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1915\u001b[0;31m                 \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1916\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '.mp4'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-03fe6e953642>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m#raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnormal_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_image_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob_string\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmark_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msave_movie_mp4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_imgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmark_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'norm_vid_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Generate blended images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-9b547c59ef7d>\u001b[0m in \u001b[0;36msave_movie_mp4\u001b[0;34m(image_array, path)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'* SAVED to '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'******'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs)\u001b[0m\n\u001b[1;32m   1198\u001b[0m                         \u001b[0;31m# TODO: See if turning off blit is really necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m                         \u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_draw_next_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m         self._frames[0].save(\n\u001b[1;32m    611\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappend_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             duration=int(1000 / self.fps))\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   1915\u001b[0m                 \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEXTENSION\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unknown file extension: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSAVE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unknown file extension: .mp4"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 160x120 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for mark_dict in marker_dicts:\n",
    "    print('* PROCESSING',mark_dict['norm_fname'])\n",
    "    print(mark_dict)\n",
    "    #raise\n",
    "    normal_imgs = gen_image_array(glob_string, mark_dict)\n",
    "    save_movie_mp4(normal_imgs,mark_dict['norm_vid_path'])\n",
    "    \n",
    "    # Generate blended images\n",
    "    #ai_imgs = gen_blended_frames(glob_string, mark_dict)\n",
    "    \n",
    "    # Generate salience masks only\n",
    "    ai_imgs = gen_salience_only_frames(glob_string, mark_dict)\n",
    "    save_movie_mp4(ai_imgs,mark_dict['ai_vid_path'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Requested MovieWriter (ffmpeg) not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavail\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ffmpeg'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-62b7ae166cb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#plot_movie_mp4(normal_imgs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_movie_mp4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mai_imgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-cfe06ec47a30>\u001b[0m in \u001b[0;36mplot_movie_mp4\u001b[0;34m(image_array)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_html5_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mto_html5_video\u001b[0;34m(self, embed_limit)\u001b[0m\n\u001b[1;32m   1347\u001b[0m                 \u001b[0;31m# We create a writer manually so that we can get the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m                 \u001b[0;31m# appropriate size for the tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1349\u001b[0;31m                 \u001b[0mWriter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwriters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.writer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m                 writer = Writer(codec='h264',\n\u001b[1;32m   1351\u001b[0m                                 \u001b[0mbitrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'animation.bitrate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deep/lib/python3.6/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             raise RuntimeError(\n\u001b[0;32m--> 173\u001b[0;31m                 'Requested MovieWriter ({}) not available'.format(name))\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Requested MovieWriter (ffmpeg) not available"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 160x120 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_movie_mp4(normal_imgs)\n",
    "plot_movie_mp4(ai_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
